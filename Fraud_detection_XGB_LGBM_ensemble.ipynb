{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_kg_hide-input": true,
    "colab": {},
    "colab_type": "code",
    "id": "iFOO-PN9LUAg"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import KFold, GroupKFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from google.colab import files\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EwGtXO0Sc8jQ"
   },
   "source": [
    "## Step 1 Get data from kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "colab_type": "code",
    "id": "XEE-migzL0Yu",
    "outputId": "00ad26a3-ba00-45d7-a998-2467bdb582ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
      "Downloading train_transaction.csv.zip to /content\n",
      " 93% 49.0M/52.5M [00:01<00:00, 19.3MB/s]\n",
      "100% 52.5M/52.5M [00:01<00:00, 47.2MB/s]\n",
      "Downloading train_identity.csv.zip to /content\n",
      "  0% 0.00/3.02M [00:00<?, ?B/s]\n",
      "100% 3.02M/3.02M [00:00<00:00, 204MB/s]\n",
      "Downloading test_transaction.csv.zip to /content\n",
      " 87% 41.0M/47.3M [00:01<00:00, 14.7MB/s]\n",
      "100% 47.3M/47.3M [00:01<00:00, 44.5MB/s]\n",
      "Downloading test_identity.csv.zip to /content\n",
      "  0% 0.00/2.97M [00:00<?, ?B/s]\n",
      "100% 2.97M/2.97M [00:00<00:00, 204MB/s]\n",
      "Downloading sample_submission.csv.zip to /content\n",
      "  0% 0.00/1.14M [00:00<?, ?B/s]\n",
      "100% 1.14M/1.14M [00:00<00:00, 202MB/s]\n",
      "kaggle.json\t\t   test_identity.csv\t     train_identity.csv\n",
      "sample_data\t\t   test_identity.csv.zip     train_identity.csv.zip\n",
      "sample_submission.csv\t   test_transaction.csv      train_transaction.csv\n",
      "sample_submission.csv.zip  test_transaction.csv.zip  train_transaction.csv.zip\n"
     ]
    }
   ],
   "source": [
    "def get_data_from_kaggle():\n",
    "  !pip install -U -q kaggle\n",
    "  with open('kaggle.json', 'w') as f:\n",
    "    f.write('{\"username\":\"YOUR USERNAME\",\"key\":\"YOUR KEY\"}') # Change to your Kaggle API key\n",
    "  !mkdir -p ~/.kaggle\n",
    "  !cp kaggle.json ~/.kaggle/\n",
    "  !kaggle competitions download -c ieee-fraud-detection\n",
    "  with ZipFile('train_identity.csv.zip', 'r') as zipObj:\n",
    "    zipObj.extractall()\n",
    "  with ZipFile('train_transaction.csv.zip', 'r') as zipObj:\n",
    "    zipObj.extractall()\n",
    "  with ZipFile('test_identity.csv.zip', 'r') as zipObj:\n",
    "    zipObj.extractall()\n",
    "  with ZipFile('test_transaction.csv.zip', 'r') as zipObj:\n",
    "    zipObj.extractall()\n",
    "  with ZipFile('sample_submission.csv.zip', 'r') as zipObj:\n",
    "    zipObj.extractall()\n",
    "  !ls\n",
    "\n",
    "get_data_from_kaggle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iGpAPWZndCOq"
   },
   "source": [
    "## Step 2 Load data and Preprocessing features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ipcSk_gELYLS"
   },
   "outputs": [],
   "source": [
    "# Column with Strings\n",
    "str_type = ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain','M1', 'M2', 'M3', 'M4','M5',\n",
    "            'M6', 'M7', 'M8', 'M9', 'id_12', 'id_15', 'id_16', 'id_23', 'id_27', 'id_28', 'id_29', 'id_30', \n",
    "            'id_31', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo']\n",
    "\n",
    "# First 53 Columns \n",
    "cols = ['TransactionID', 'TransactionDT', 'TransactionAmt',\n",
    "       'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6',\n",
    "       'addr1', 'addr2', 'dist1', 'dist2', 'P_emaildomain', 'R_emaildomain',\n",
    "       'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11',\n",
    "       'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8',\n",
    "       'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'M1', 'M2', 'M3', 'M4',\n",
    "       'M5', 'M6', 'M7', 'M8', 'M9']\n",
    "\n",
    "# \"V\" Columns to load decided by correlation analysis \n",
    "# See this notebook: https://www.kaggle.com/cdeotte/eda-for-columns-v-and-id\n",
    "\n",
    "v =  [1, 3, 4, 6, 8, 11]\n",
    "v += [13, 14, 17, 20, 23, 26, 27, 30]\n",
    "v += [36, 37, 40, 41, 44, 47, 48]\n",
    "v += [54, 56, 59, 62, 65, 67, 68, 70]\n",
    "v += [76, 78, 80, 82, 86, 88, 89, 91]\n",
    "\n",
    "v += [107, 108, 111, 115, 117, 120, 121, 123] # maybe group, no NAN\n",
    "v += [124, 127, 129, 130, 136] # relates to groups, no NAN\n",
    "\n",
    "# lots of NaNs below\n",
    "v += [138, 139, 142, 147, 156, 162] #b1\n",
    "v += [165, 160, 166] #b1\n",
    "v += [178, 176, 173, 182] #b2\n",
    "v += [187, 203, 205, 207, 215] #b2\n",
    "v += [169, 171, 175, 180, 185, 188, 198, 210, 209] #b2\n",
    "v += [218, 223, 224, 226, 228, 229, 235] #b3\n",
    "v += [240, 258, 257, 253, 252, 260, 261] #b3\n",
    "v += [264, 266, 267, 274, 277] #b3\n",
    "v += [220, 221, 234, 238, 250, 271] #b3\n",
    "\n",
    "v += [294, 284, 285, 286, 291, 297] # relates to grous, no NAN\n",
    "v += [303, 305, 307, 309, 310, 320] # relates to groups, no NAN\n",
    "v += [281, 283, 289, 296, 301, 314] # relates to groups, no NAN\n",
    "\n",
    "cols += ['V'+str(x) for x in v]\n",
    "\n",
    "dtypes = {}\n",
    "for c in cols+['id_0'+str(x) for x in range(1,10)]+['id_'+str(x) for x in range(10,34)]: \n",
    "    dtypes[c] = 'float32'\n",
    "for c in str_type: \n",
    "    dtypes[c] = 'category'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Kxj5FySzLUAl",
    "outputId": "f86dd948-6792-4fb7-fccc-622d5c58bb86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape (590540, 213) test shape (506691, 213)\n"
     ]
    }
   ],
   "source": [
    "# Load train\n",
    "X_train = pd.read_csv('train_transaction.csv', index_col='TransactionID', dtype=dtypes, usecols=cols+['isFraud'])\n",
    "train_id = pd.read_csv('train_identity.csv', index_col='TransactionID', dtype=dtypes)\n",
    "X_train = X_train.merge(train_id, how='left', left_index=True, right_index=True)\n",
    "y_train = X_train['isFraud'].copy()\n",
    "\n",
    "# Load test\n",
    "X_test = pd.read_csv('test_transaction.csv',index_col='TransactionID', dtype=dtypes, usecols=cols)\n",
    "test_id = pd.read_csv('test_identity.csv',index_col='TransactionID', dtype=dtypes)\n",
    "X_test = X_test.merge(test_id, how='left', left_index=True, right_index=True)\n",
    "\n",
    "del train_id, test_id, X_train['isFraud']\n",
    "\n",
    "print('Train shape',X_train.shape,'test shape',X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_kg_hide-input": true,
    "colab": {},
    "colab_type": "code",
    "id": "DJZGOHZzLUAz"
   },
   "outputs": [],
   "source": [
    "# Normalize \"D\" Column\n",
    "for i in range(1,16):\n",
    "    if i in [1,2,3,5,9]: continue\n",
    "    X_train['D'+str(i)] =  X_train['D'+str(i)] - X_train.TransactionDT/np.float32(24*60*60)\n",
    "    X_test['D'+str(i)] = X_test['D'+str(i)] - X_test.TransactionDT/np.float32(24*60*60) \n",
    "\n",
    "for i,f in enumerate(X_train.columns):\n",
    "# Factorize categorical features\n",
    "    if (np.str(X_train[f].dtype)=='category')|(X_train[f].dtype=='object'): \n",
    "        df_comb = pd.concat([X_train[f],X_test[f]],axis=0)\n",
    "        df_comb,_ = df_comb.factorize(sort=True)\n",
    "        if df_comb.max()>32000: print(f,'needs int32')\n",
    "        X_train[f] = df_comb[:len(X_train)].astype('int16')\n",
    "        X_test[f] = df_comb[len(X_train):].astype('int16')    \n",
    "# Make all numerical features positive and set NaN to -1\n",
    "    elif f not in ['TransactionAmt','TransactionDT']:\n",
    "        mn = np.min((X_train[f].min(),X_test[f].min()))\n",
    "        X_train[f] -= np.float32(mn) # minus the min value\n",
    "        X_test[f] -= np.float32(mn)  # minus the min value\n",
    "        X_train[f].fillna(-1,inplace=True)\n",
    "        X_test[f].fillna(-1,inplace=True)\n",
    "\n",
    "# Add a new feature: cents in TransactionAmt\n",
    "X_train['cents'] = (X_train['TransactionAmt'] - np.floor(X_train['TransactionAmt'])).astype('float32')\n",
    "X_test['cents'] = (X_test['TransactionAmt'] - np.floor(X_test['TransactionAmt'])).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cP_F9c16W4Pq"
   },
   "outputs": [],
   "source": [
    "cols = list( X_train.columns )\n",
    "cols.remove('TransactionDT')\n",
    "for c in ['D6','D7','D8','D9','D12','D13','D14']:\n",
    "    cols.remove(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bh163wUDdbq6"
   },
   "source": [
    "## Step 3 Local Solid CV for Hyperparameter Tuning (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oW_T_AGwdmK5"
   },
   "outputs": [],
   "source": [
    "do_local_cv_xgb = False # True or False\n",
    "do_local_cv_lgb = False # True or False\n",
    "\n",
    "if do_local_cv_xgb:\n",
    "\n",
    "  idxT = X_train.index[:3*len(X_train)//4] # 3/4 as training set\n",
    "  idxV = X_train.index[3*len(X_train)//4:] # 1/4 as validation set\n",
    "\n",
    "  model_xgb = xgb.XGBClassifier( \n",
    "              n_estimators = 200,\n",
    "              max_depth = 12, \n",
    "              learning_rate = 0.02, \n",
    "              subsample = 0.8,\n",
    "              colsample_bytree = 0.4, \n",
    "              missing = -1, \n",
    "              eval_metric = 'auc',\n",
    "              tree_method = 'gpu_hist'\n",
    "  )\n",
    "  hist = model_xgb.fit(X_train.loc[idxT,cols], y_train[idxT], \n",
    "                      eval_set=[(X_train.loc[idxV,cols],y_train[idxV])],\n",
    "                      verbose=50, early_stopping_rounds=100)\n",
    "  del model_xgb\n",
    "\n",
    "\n",
    "if do_local_cv_xgb:\n",
    "\n",
    "  idxT = X_train.index[:3*len(X_train)//4]\n",
    "  idxV = X_train.index[3*len(X_train)//4:]\n",
    "\n",
    "  model_lgb = lgb.LGBMClassifier(\n",
    "              num_leaves = 144,\n",
    "              max_depth = 12,\n",
    "              learning_rate = 0.02, \n",
    "              n_estimators = 1000,  \n",
    "              bagging_fraction = 0.8,\n",
    "              bagging_freq = 5, \n",
    "              feature_fraction = 0.9,\n",
    "              n_jobs = -1,\n",
    "              missing = -1,\n",
    "              verbose = -1) \n",
    "\n",
    "  hist = model_lgb.fit(X_train.loc[idxT,cols], y_train[idxT], \n",
    "                      eval_set=[(X_train.loc[idxV,cols],y_train[idxV])],\n",
    "                      verbose=50, early_stopping_rounds=100, eval_metric='auc')\n",
    "  del model_lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-RujMchadGAf"
   },
   "source": [
    "## Step 4 Ensemble XGB and LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Jp3HtBaTLUBU",
    "outputId": "3569b743-3bfe-4093-c01b-331c26ce800e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Fold  0\n",
      "samples for train =  472431 samples for validation =  118109\n",
      "--- Traing XGB \n",
      "[0]\tvalidation_0-auc:0.83774\n",
      "Will train until validation_0-auc hasn't improved in 100 rounds.\n",
      "[50]\tvalidation_0-auc:0.893765\n",
      "[100]\tvalidation_0-auc:0.908615\n",
      "[150]\tvalidation_0-auc:0.923749\n",
      "[199]\tvalidation_0-auc:0.934384\n",
      "--- Training LGBM \n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.0952255\tvalid_0's auc: 0.898925\n",
      "[100]\tvalid_0's binary_logloss: 0.0826363\tvalid_0's auc: 0.9173\n",
      "[150]\tvalid_0's binary_logloss: 0.076306\tvalid_0's auc: 0.928619\n",
      "[200]\tvalid_0's binary_logloss: 0.0716261\tvalid_0's auc: 0.938581\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[200]\tvalid_0's binary_logloss: 0.0716261\tvalid_0's auc: 0.938581\n",
      "=== Fold  1\n",
      "samples for train =  472431 samples for validation =  118109\n",
      "--- Traing XGB \n",
      "[0]\tvalidation_0-auc:0.85461\n",
      "Will train until validation_0-auc hasn't improved in 100 rounds.\n",
      "[50]\tvalidation_0-auc:0.902123\n",
      "[100]\tvalidation_0-auc:0.916385\n",
      "[150]\tvalidation_0-auc:0.930558\n",
      "[199]\tvalidation_0-auc:0.940917\n",
      "--- Training LGBM \n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.0934642\tvalid_0's auc: 0.908749\n",
      "[100]\tvalid_0's binary_logloss: 0.0802612\tvalid_0's auc: 0.924402\n",
      "[150]\tvalid_0's binary_logloss: 0.0737952\tvalid_0's auc: 0.935858\n",
      "[200]\tvalid_0's binary_logloss: 0.0691466\tvalid_0's auc: 0.945701\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[200]\tvalid_0's binary_logloss: 0.0691466\tvalid_0's auc: 0.945701\n",
      "=== Fold  2\n",
      "samples for train =  472432 samples for validation =  118108\n",
      "--- Traing XGB \n",
      "[0]\tvalidation_0-auc:0.848821\n",
      "Will train until validation_0-auc hasn't improved in 100 rounds.\n",
      "[50]\tvalidation_0-auc:0.895954\n",
      "[100]\tvalidation_0-auc:0.910846\n",
      "[150]\tvalidation_0-auc:0.925372\n",
      "[199]\tvalidation_0-auc:0.936349\n",
      "--- Training LGBM \n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.0953854\tvalid_0's auc: 0.90133\n",
      "[100]\tvalid_0's binary_logloss: 0.0823711\tvalid_0's auc: 0.919468\n",
      "[150]\tvalid_0's binary_logloss: 0.0754696\tvalid_0's auc: 0.932444\n",
      "[200]\tvalid_0's binary_logloss: 0.0710681\tvalid_0's auc: 0.94074\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[200]\tvalid_0's binary_logloss: 0.0710681\tvalid_0's auc: 0.94074\n",
      "=== Fold  3\n",
      "samples for train =  472433 samples for validation =  118107\n",
      "--- Traing XGB \n",
      "[0]\tvalidation_0-auc:0.840899\n",
      "Will train until validation_0-auc hasn't improved in 100 rounds.\n",
      "[50]\tvalidation_0-auc:0.891798\n",
      "[100]\tvalidation_0-auc:0.90699\n",
      "[150]\tvalidation_0-auc:0.922798\n",
      "[199]\tvalidation_0-auc:0.934617\n",
      "--- Training LGBM \n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.0958207\tvalid_0's auc: 0.898124\n",
      "[100]\tvalid_0's binary_logloss: 0.0832212\tvalid_0's auc: 0.915647\n",
      "[150]\tvalid_0's binary_logloss: 0.0767026\tvalid_0's auc: 0.928604\n",
      "[200]\tvalid_0's binary_logloss: 0.0720558\tvalid_0's auc: 0.938169\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[200]\tvalid_0's binary_logloss: 0.0720558\tvalid_0's auc: 0.938169\n",
      "=== Fold  4\n",
      "samples for train =  472433 samples for validation =  118107\n",
      "--- Traing XGB \n",
      "[0]\tvalidation_0-auc:0.848618\n",
      "Will train until validation_0-auc hasn't improved in 100 rounds.\n",
      "[50]\tvalidation_0-auc:0.892527\n",
      "[100]\tvalidation_0-auc:0.907628\n",
      "[150]\tvalidation_0-auc:0.923498\n",
      "[199]\tvalidation_0-auc:0.935218\n",
      "--- Training LGBM \n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.0953429\tvalid_0's auc: 0.897487\n",
      "[100]\tvalid_0's binary_logloss: 0.0824245\tvalid_0's auc: 0.91809\n",
      "[150]\tvalid_0's binary_logloss: 0.0759016\tvalid_0's auc: 0.930585\n",
      "[200]\tvalid_0's binary_logloss: 0.0713817\tvalid_0's auc: 0.939753\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[200]\tvalid_0's binary_logloss: 0.0713817\tvalid_0's auc: 0.939753\n"
     ]
    }
   ],
   "source": [
    "pred_xgb = np.zeros(len(X_test))\n",
    "pred_lgb = np.zeros(len(X_test))\n",
    "\n",
    "model_xgb = xgb.XGBClassifier(\n",
    "            n_estimators = 200,\n",
    "            max_depth = 12,\n",
    "            learning_rate = 0.02,\n",
    "            subsample = 0.8,\n",
    "            colsample_bytree = 0.4,\n",
    "            eval_metric = 'auc',\n",
    "            missing = -1,\n",
    "            tree_method='gpu_hist')  \n",
    "\n",
    "model_lgb = lgb.LGBMClassifier(\n",
    "            num_leaves = 144,\n",
    "            max_depth = 12,\n",
    "            learning_rate = 0.02, \n",
    "            n_estimators = 200,  \n",
    "            bagging_fraction = 0.8,\n",
    "            bagging_freq = 5, \n",
    "            feature_fraction = 0.9,\n",
    "            n_jobs = -1,\n",
    "            missing = -1,\n",
    "            verbose = -1) \n",
    "    \n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "for i, (idxT, idxV) in enumerate(skf.split(X_train, y_train)):\n",
    "\n",
    "    print('=== Fold ', i)\n",
    "    print('samples for train = ', len(idxT), 'samples for validation = ', len(idxV))\n",
    "\n",
    "    print('--- Training XGB ')\n",
    "    model_xgb.fit(X_train[cols].iloc[idxT], y_train.iloc[idxT], \n",
    "                        eval_set=[(X_train[cols].iloc[idxV],y_train.iloc[idxV])],\n",
    "                        verbose=50, early_stopping_rounds=100)\n",
    "    pred_xgb += model_xgb.predict_proba(X_test[cols])[:,1]/skf.n_splits\n",
    "\n",
    "    print('--- Training LGBM ')\n",
    "    model_lgb.fit(X_train[cols].iloc[idxT], y_train.iloc[idxT], \n",
    "                        eval_set=[(X_train[cols].iloc[idxV],y_train.iloc[idxV])],\n",
    "                        verbose=50, early_stopping_rounds=100, eval_metric='auc')\n",
    "    \n",
    "    pred_lgb += model_lgb.predict_proba(X_test[cols])[:,1]/skf.n_splits\n",
    "\n",
    "preds = 0.5 * (pred_lgb + pred_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AN958JgRc4Lk"
   },
   "source": [
    "## Final Step: Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K8fcH9pILUBb"
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "sample_submission.isFraud = preds\n",
    "sample_submission.to_csv('my_submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Fraud_detection_XGB_LGBM_ensemble.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
